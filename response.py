import os
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEndpoint
from huggingface_hub import login

# Load environment variables
load_dotenv()

# Log in to Hugging Face Hub using your API key
huggingface_api_key = os.getenv("HUGGINGFACE_TOKEN")
if huggingface_api_key:
    login(huggingface_api_key)
else:
    raise ValueError("HUGGINGFACE_TOKEN is not set in the environment variables.")

# Initialize the Hugging Face endpoint
repo_id = "mistralai/Mixtral-8x7B-Instruct-v0.1"
llm = HuggingFaceEndpoint(
    repo_id=repo_id, 
    temperature=0.8, 
    top_k=5, 
    huggingfacehub_api_token=huggingface_api_key
)

def generate_response(chunks, query):
    """
    Generate a response based on the most relevant chunks from the PDF and the query.
    
    Args:
        chunks (list of str): List of relevant chunks retrieved from the PDF.
        query (str): User's query.
    
    Returns:
        str: Response generated by the model.
    """
    # Combine the relevant chunks and the query to provide context to the model
    context = "\n".join(chunks)
    prompt = f"Based on the following document context:\n{context}\n\nAnswer the query: {query}"
    
    # Generate a response
    response = llm(prompt)
    return response
